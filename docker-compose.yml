services: 
  bot_service:
    build:
      dockerfile: Dockerfile
      context: bot-server/
    ports:
      - "5003:5003"
    depends_on:
      values_service:
        condition: service_healthy
      schema_service:
        condition: service_healthy
      ollama_service:
        condition: service_healthy
    restart: always
  values_service:
    build:
      dockerfile: Dockerfile
      context: values-server/
    volumes:
      - ./data/values:/data/values
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:5002/health')"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 5s
    restart: always
  schema_service:
    build:
      dockerfile: Dockerfile
      context: schema-server/
    volumes:
      - ./data/schemas:/data/schemas
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:5001/health')"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 5s
    restart: always
  ollama_service:
    image: ollama/ollama:latest
    volumes:
      - models:/.ollama/models
    entrypoint: ["/bin/sh", "-c"]
    command: >
      "ollama serve &
      sleep 5 &&
      ollama pull qwen3:0.6b &&
      ollama pull llama3.1:8b &&
      wait"
    # UN-COMMENT FOR PERFORMANCE OPTIMIZATION IF THERE IS NVIDIA GPU ON THE HOST
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]
    healthcheck:
      test: ["CMD-SHELL", "ollama list | grep -q 'qwen3:0.6b' && ollama list | grep -q 'llama3.1:8b'"]
      interval: 30s
      timeout: 10s
      retries: 20
      start_period: 120s
    restart: always
volumes:
  models:
    driver: local
